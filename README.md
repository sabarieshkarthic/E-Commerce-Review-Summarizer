# ğŸ›’ E-Commerce Review Summarizer (Transformer)

This project implements a **Transformer-based sequence-to-sequence model** for **summarizing e-commerce product reviews**, built **entirely from scratch using Python and NumPy**.

The implementation includes custom encoderâ€“decoder blocks, attention mechanisms, embeddings, loss computation, backpropagation, and inference logic without relying on high-level deep learning frameworks.

---

## ğŸ“Œ Project Overview

The system converts long product reviews into short summaries using a **Transformer encoderâ€“decoder architecture**.

The pipeline consists of:
- Vocabulary and embedding construction
- Encoder stack with self-attention
- Decoder stack with masked self-attention and cross-attention

---

## ğŸ“‚ Project Structure
- ecommerce-review-summarizer/
- â”‚
- â”œâ”€â”€ Add_and_Norm.py             
- â”œâ”€â”€ Cross_Attention.py           
- â”œâ”€â”€ CrossMultiHead.py           
- â”œâ”€â”€ Decoder.py                   
- â”œâ”€â”€ Encoder.py                  
- â”œâ”€â”€ FeedForward.py               
- â”œâ”€â”€ inputembeeding.py            
- â”œâ”€â”€ LinearAndSoftmax.py          
- â”œâ”€â”€ Masked_Multi_Head.py         
- â”œâ”€â”€ Masked_Single_Attention.py   
- â”œâ”€â”€ Multi_Head_Attention.py      
- â”œâ”€â”€ Positional_encoding.py      
- â”œâ”€â”€ Single_Head_Attention.py     
- â”œâ”€â”€ Transformer.py               
- â”œâ”€â”€ Vocublary_matrix.py         
- â””â”€â”€ README.md

---

## ğŸ§  Model Architecture

The model follows a **standard Transformer encoderâ€“decoder design**.

### Encoder
- Input embedding + positional encoding  
- Multi-head self-attention  
- Feedforward network  
- Residual connections and layer normalization  

### Decoder
- Masked self-attention  
- Cross-attention with encoder outputs  
- Feedforward network  
- Residual connections and layer normalization  

 

---

## ğŸ” Text Preprocessing

- Lowercasing  
- Tokenization by whitespace  
- Vocabulary indexing  
- Special tokens:
  - `<start>`
  - `<end>`
  - `<pad>`
  - `<unk>`
- Sequence padding and truncation  
- Shared vocabulary for encoder and decoder  

---

## ğŸ” Training Pipeline

- Sequence-to-sequence training   
- Token-level cross-entropy loss  
- Manual backpropagation  
- Parameter updates using **gradient descent**  

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|---------|------|
| Encoder blocks | 1 |
| Decoder blocks | 1 |
| Optimizer | Gradient Descent |
| Learning rate | 0.01 |
| Epochs | Up to 2000 |
| Loss function | Cross-Entropy |

---
## References:
  - "Attention Is All You Need"
  - "The Illustrated Transformer - "Alammar J"




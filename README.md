# ğŸ›’ E-Commerce Review Summarizer (Transformer)

This project implements a **Transformer-based sequence-to-sequence model** for **summarizing e-commerce product reviews**, built **entirely from scratch using Python and NumPy**.

The implementation includes custom encoderâ€“decoder blocks, attention mechanisms, embeddings, loss computation, backpropagation, and inference logic without relying on high-level deep learning frameworks.

---

## ğŸ“Œ Project Overview

The system converts long product reviews into short summaries using a **Transformer encoderâ€“decoder architecture**.

The pipeline consists of:
- Vocabulary and embedding construction
- Encoder stack with self-attention
- Decoder stack with masked self-attention and cross-attention

---

## ğŸ“‚ Project Structure
ecommerce-review-summarizer/
â”‚
â”œâ”€â”€ Add_and_Norm.py              # Residual connection + layer normalization
â”œâ”€â”€ Cross_Attention.py           # Encoderâ€“decoder attention
â”œâ”€â”€ CrossMultiHead.py            # Multi-head cross-attention
â”œâ”€â”€ Decoder.py                   # Transformer decoder block
â”œâ”€â”€ Encoder.py                   # Transformer encoder block
â”œâ”€â”€ FeedForward.py               # Position-wise feedforward network
â”œâ”€â”€ inputembeeding.py            # Token + positional embeddings
â”œâ”€â”€ LinearAndSoftmax.py          # Output projection and softmax
â”œâ”€â”€ Masked_Multi_Head.py         # Masked multi-head attention
â”œâ”€â”€ Masked_Single_Attention.py   # Masked single-head attention
â”œâ”€â”€ Multi_Head_Attention.py      # Multi-head self-attention
â”œâ”€â”€ Positional_encoding.py       # Positional encoding
â”œâ”€â”€ Single_Head_Attention.py     # Single-head attention
â”œâ”€â”€ Transformer.py               # Full Transformer model
â”œâ”€â”€ Vocublary_matrix.py          # Vocabulary and shared embedding matrix
â””â”€â”€ README.md

---

## ğŸ§  Model Architecture

The model follows a **standard Transformer encoderâ€“decoder design**.

### Encoder
- Input embedding + positional encoding  
- Multi-head self-attention  
- Feedforward network  
- Residual connections and layer normalization  

### Decoder
- Masked self-attention  
- Cross-attention with encoder outputs  
- Feedforward network  
- Residual connections and layer normalization  

### Output
- Linear projection  
- Softmax over vocabulary  

---

## ğŸ” Text Preprocessing

- Lowercasing  
- Tokenization by whitespace  
- Vocabulary indexing  
- Special tokens:
  - `<start>`
  - `<end>`
  - `<pad>`
  - `<unk>`
- Sequence padding and truncation  
- Shared vocabulary for encoder and decoder  

---

## ğŸ” Training Pipeline

- Sequence-to-sequence training   
- Token-level cross-entropy loss  
- Manual backpropagation  
- Parameter updates using **gradient descent**  

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|---------|------|
| Encoder blocks | 1 |
| Decoder blocks | 1 |
| Optimizer | Gradient Descent |
| Learning rate | 0.01 |
| Epochs | Up to 2000 |
| Loss function | Cross-Entropy |

---
## References:
  - title: "Attention Is All You Need"
  - title: "The Illustrated Transformer - "Alammar J"



